perfect question ğŸ‘ â€” once your SLM finishes training, youâ€™ve basically got a tiny local language model sitting in memory.
now you can generate text, save it, reload it, or fine-tune further.
hereâ€™s the complete post-training workflow explained step-by-step ğŸ‘‡


---

ğŸ§  1ï¸âƒ£ understand what just happened

after training, the script already prints:

=== SAMPLE ===
To be or not to be, the mind to suffer...
==============

thatâ€™s the model generating text conditioned on the --seed string.

so youâ€™ve already seen it work â€” thatâ€™s a forward pass using its learned weights.


---

ğŸ’¾ 2ï¸âƒ£ save the trained model

add this near the end of slm.py (just above if __name__ == "__main__":)

def save_model(model, path="slm.pth"):
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

then after training:

save_model(model)

this saves the trained weights (~100â€“200 KB) locally.


---

ğŸ”„ 3ï¸âƒ£ load it later for generation

create a small script generate.py:

import torch
from slm import TinyGPT, CharData, load_text

device = "cpu"
text = load_text()
data = CharData(text)

model = TinyGPT(vocab_size=data.vocab_size)
model.load_state_dict(torch.load("slm.pth", map_location=device))
model.eval()

seed = "Audit"
inp = data.encode(seed).unsqueeze(0)
out = model.generate(inp.to(device), max_new_tokens=300)
print(data.decode(out[0].tolist()))

run:

python generate.py

âœ… this reuses your trained weights and instantly generates new text without retraining.


---

ğŸ§ª 4ï¸âƒ£ experiment further

you can now:

change corpus (replace input.txt and retrain)

tune hyperparameters, e.g.

python slm.py --train-steps 1000 --n-layer 3 --n-head 4

save checkpoints after each epoch if you want multiple versions

plot training loss (log and visualize over steps)



---

âš¡ 5ï¸âƒ£ optional: use interactively

add a short prompt loop inside generate.py:

while True:
    seed = input("You: ")
    if seed.lower() in ["quit", "exit"]:
        break
    out = model.generate(data.encode(seed).unsqueeze(0), max_new_tokens=150)
    print("Model:", data.decode(out[0].tolist()))

now you have your own chat-like mini LLM running 100% offline ğŸ§©


---

would you like me to show how to extend this so it accepts multi-sentence prompts (like a tiny conversation memory or audit Q&A)?

